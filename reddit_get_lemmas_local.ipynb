{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f7e26c2",
   "metadata": {},
   "source": [
    "# Tokenize and Gather Lemmas for Cleaned Reddit Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2249ad89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "! pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db4c121",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee72f4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import time\n",
    "\n",
    "# NOTE: stopwords include pronouns! TODO: make custom stop words list?\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0f19b8",
   "metadata": {},
   "source": [
    "* Modify list of Stop Words to remove words of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c18f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "keep_stop = set(['she', 'her', 'hers', 'herself'])\n",
    "mod_stop_words = stop_words - keep_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3524986",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmtzr = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b3825917",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_corpus = pickle.load(open(\"reddit_full_cleaned.pickle\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1a770a",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a75c3a",
   "metadata": {},
   "source": [
    "* Turn tokenized comment into Lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "db52d0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_comments = comments_corpus['cleaned_text']\n",
    "\n",
    "lemmatized = [[lmtzr.lemmatize(word) for word in word_tokenize(s)]\n",
    "              for s in cleaned_comments]\n",
    "\n",
    "empty_list = list()\n",
    "for lemmas in lemmatized:\n",
    "    empty_list.append(' '.join(lemmas))\n",
    "    \n",
    "\n",
    "comments_corpus['lemmas'] = lemmatized\n",
    "comments_corpus['lemmas_str'] = empty_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b381aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b81ce0",
   "metadata": {},
   "source": [
    "* Use Lemmatized comment to detremine sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a6eab61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Negative\n",
    "comments_corpus['neg'] = [analyzer.polarity_scores(v)['neg'] for v in comments_corpus['lemmas_str']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d03441db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compound\n",
    "comments_corpus['compound'] = [analyzer.polarity_scores(v)['compound'] for v in comments_corpus['lemmas_str']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "75af9d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positive\n",
    "comments_corpus['pos'] = [analyzer.polarity_scores(v)['pos'] for v in comments_corpus['lemmas_str']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4c71834c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>comment</th>\n",
       "      <th>time created</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>lemmas_str</th>\n",
       "      <th>neg</th>\n",
       "      <th>compound</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TheRedPill</td>\n",
       "      <td>Why the fuck would you be friends with an ex?</td>\n",
       "      <td>1451606442</td>\n",
       "      <td>fuck would friends ex</td>\n",
       "      <td>[fuck, would, friend, ex]</td>\n",
       "      <td>fuck would friend ex</td>\n",
       "      <td>0.402</td>\n",
       "      <td>-0.0772</td>\n",
       "      <td>0.368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TheRedPill</td>\n",
       "      <td>Great story full of RP truths. Thanks. Conside...</td>\n",
       "      <td>1451606479</td>\n",
       "      <td>great story full rp truths thanks consider mak...</td>\n",
       "      <td>[great, story, full, rp, truth, thanks, consid...</td>\n",
       "      <td>great story full rp truth thanks consider maki...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.8519</td>\n",
       "      <td>0.482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TheRedPill</td>\n",
       "      <td>Lets suss this out. \\n\\nFor one; she's either ...</td>\n",
       "      <td>1451606497</td>\n",
       "      <td>lets suss one shes either got high paying job ...</td>\n",
       "      <td>[let, sus, one, shes, either, got, high, payin...</td>\n",
       "      <td>let sus one shes either got high paying job cl...</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.0790</td>\n",
       "      <td>0.068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TheRedPill</td>\n",
       "      <td>Sounds like you may need some more treatment f...</td>\n",
       "      <td>1451606532</td>\n",
       "      <td>sounds like may need treatment anxiety crucial...</td>\n",
       "      <td>[sound, like, may, need, treatment, anxiety, c...</td>\n",
       "      <td>sound like may need treatment anxiety crucial ...</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.8020</td>\n",
       "      <td>0.243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TheRedPill</td>\n",
       "      <td>hack her phone or fb, its the only way you'll ...</td>\n",
       "      <td>1451606539</td>\n",
       "      <td>hack her phone fb way youll truly know think h...</td>\n",
       "      <td>[hack, her, phone, fb, way, youll, truly, know...</td>\n",
       "      <td>hack her phone fb way youll truly know think h...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.6124</td>\n",
       "      <td>0.357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574679</th>\n",
       "      <td>technews</td>\n",
       "      <td>PLEASE! I GET LIKE 3Mb/s I NEEEED THIS</td>\n",
       "      <td>1483147728</td>\n",
       "      <td>please get like 3mbs neeeed</td>\n",
       "      <td>[please, get, like, 3mbs, neeeed]</td>\n",
       "      <td>please get like 3mbs neeeed</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.5859</td>\n",
       "      <td>0.615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574680</th>\n",
       "      <td>technews</td>\n",
       "      <td>Took some screenshots if it gets removed avail...</td>\n",
       "      <td>1483189543</td>\n",
       "      <td>took screenshots gets removed available httpii...</td>\n",
       "      <td>[took, screenshots, get, removed, available, h...</td>\n",
       "      <td>took screenshots get removed available httpiim...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574681</th>\n",
       "      <td>technews</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>1483199435</td>\n",
       "      <td>removed</td>\n",
       "      <td>[removed]</td>\n",
       "      <td>removed</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574682</th>\n",
       "      <td>technews</td>\n",
       "      <td>This is the best tl;dr I could make, [original...</td>\n",
       "      <td>1483205392</td>\n",
       "      <td>best tldr could make originalhttpwwwreuterscom...</td>\n",
       "      <td>[best, tldr, could, make, originalhttpwwwreute...</td>\n",
       "      <td>best tldr could make originalhttpwwwreuterscom...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.9231</td>\n",
       "      <td>0.144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574683</th>\n",
       "      <td>technews</td>\n",
       "      <td>Good, LCD prices have been stagnant too long d...</td>\n",
       "      <td>1483215756</td>\n",
       "      <td>good lcd prices stagnant long due lack competi...</td>\n",
       "      <td>[good, lcd, price, stagnant, long, due, lack, ...</td>\n",
       "      <td>good lcd price stagnant long due lack competit...</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.1531</td>\n",
       "      <td>0.169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>574684 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         subreddit                                            comment  \\\n",
       "0       TheRedPill     Why the fuck would you be friends with an ex?    \n",
       "1       TheRedPill  Great story full of RP truths. Thanks. Conside...   \n",
       "2       TheRedPill  Lets suss this out. \\n\\nFor one; she's either ...   \n",
       "3       TheRedPill  Sounds like you may need some more treatment f...   \n",
       "4       TheRedPill  hack her phone or fb, its the only way you'll ...   \n",
       "...            ...                                                ...   \n",
       "574679    technews             PLEASE! I GET LIKE 3Mb/s I NEEEED THIS   \n",
       "574680    technews  Took some screenshots if it gets removed avail...   \n",
       "574681    technews                                          [removed]   \n",
       "574682    technews  This is the best tl;dr I could make, [original...   \n",
       "574683    technews  Good, LCD prices have been stagnant too long d...   \n",
       "\n",
       "        time created                                       cleaned_text  \\\n",
       "0         1451606442                              fuck would friends ex   \n",
       "1         1451606479  great story full rp truths thanks consider mak...   \n",
       "2         1451606497  lets suss one shes either got high paying job ...   \n",
       "3         1451606532  sounds like may need treatment anxiety crucial...   \n",
       "4         1451606539  hack her phone fb way youll truly know think h...   \n",
       "...              ...                                                ...   \n",
       "574679    1483147728                        please get like 3mbs neeeed   \n",
       "574680    1483189543  took screenshots gets removed available httpii...   \n",
       "574681    1483199435                                            removed   \n",
       "574682    1483205392  best tldr could make originalhttpwwwreuterscom...   \n",
       "574683    1483215756  good lcd prices stagnant long due lack competi...   \n",
       "\n",
       "                                                   lemmas  \\\n",
       "0                               [fuck, would, friend, ex]   \n",
       "1       [great, story, full, rp, truth, thanks, consid...   \n",
       "2       [let, sus, one, shes, either, got, high, payin...   \n",
       "3       [sound, like, may, need, treatment, anxiety, c...   \n",
       "4       [hack, her, phone, fb, way, youll, truly, know...   \n",
       "...                                                   ...   \n",
       "574679                  [please, get, like, 3mbs, neeeed]   \n",
       "574680  [took, screenshots, get, removed, available, h...   \n",
       "574681                                          [removed]   \n",
       "574682  [best, tldr, could, make, originalhttpwwwreute...   \n",
       "574683  [good, lcd, price, stagnant, long, due, lack, ...   \n",
       "\n",
       "                                               lemmas_str    neg  compound  \\\n",
       "0                                    fuck would friend ex  0.402   -0.0772   \n",
       "1       great story full rp truth thanks consider maki...  0.000    0.8519   \n",
       "2       let sus one shes either got high paying job cl...  0.076    0.0790   \n",
       "3       sound like may need treatment anxiety crucial ...  0.046    0.8020   \n",
       "4       hack her phone fb way youll truly know think h...  0.000    0.6124   \n",
       "...                                                   ...    ...       ...   \n",
       "574679                        please get like 3mbs neeeed  0.000    0.5859   \n",
       "574680  took screenshots get removed available httpiim...  0.000    0.0000   \n",
       "574681                                            removed  0.000    0.0000   \n",
       "574682  best tldr could make originalhttpwwwreuterscom...  0.000    0.9231   \n",
       "574683  good lcd price stagnant long due lack competit...  0.134    0.1531   \n",
       "\n",
       "          pos  \n",
       "0       0.368  \n",
       "1       0.482  \n",
       "2       0.068  \n",
       "3       0.243  \n",
       "4       0.357  \n",
       "...       ...  \n",
       "574679  0.615  \n",
       "574680  0.000  \n",
       "574681  0.000  \n",
       "574682  0.144  \n",
       "574683  0.169  \n",
       "\n",
       "[574684 rows x 9 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b85d59",
   "metadata": {},
   "source": [
    "* Change utc time created to DateTime object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18be8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_time(df):\n",
    "    '''\n",
    "    converts utc code to datetime object\n",
    "    '''\n",
    "    converted_times = []\n",
    "    for time in df['time created']:\n",
    "        converted_times.append(datetime.fromtimestamp(float(time), tz=timezone.utc))\n",
    "        \n",
    "    df['time created'] = converted_times\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dcdb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "change_time(comments_corpus)\n",
    "comments_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edeb9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('reddit_comments_with_sentiment.pickle', 'wb') as handle:\n",
    "    a = pickle.dump(comments_corpus, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('reddit_comments_with_sentiment.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
